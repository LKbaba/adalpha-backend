"""
Stream Manager Service for SSE (Server-Sent Events).

Manages real-time data streaming from Kafka to frontend clients
using Server-Sent Events (SSE).

Architecture:
- Kafka Consumer reads from vks-scores and market-stream topics
- StreamManager broadcasts messages to all connected SSE clients
- Heartbeat keeps connections alive
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Set, Optional, AsyncGenerator
from dataclasses import dataclass, field
import threading
from queue import Queue, Empty

from confluent_kafka import Consumer, KafkaError

from app.config import get_settings
from app.services.kafka_client import kafka_client
from app.services.adaptive_trend_scorer import adaptive_trend_scorer
from app.services.smart_history_store import smart_history_store

logger = logging.getLogger(__name__)


@dataclass
class SSEClient:
    """Represents a connected SSE client."""
    client_id: str
    connected_at: datetime = field(default_factory=datetime.utcnow)
    last_event_id: Optional[str] = None
    topics: Set[str] = field(default_factory=set)


class StreamManager:
    """
    Manages SSE connections and Kafka message broadcasting.

    Features:
    - Multiple client support
    - Automatic heartbeat (every 15 seconds)
    - Kafka consumer for real-time VKS updates
    - Message queue for each client
    """

    def __init__(self):
        self._settings = get_settings()
        self._clients: Dict[str, Queue] = {}
        self._client_info: Dict[str, SSEClient] = {}
        self._running = False
        self._consumer_task: Optional[asyncio.Task] = None
        self._heartbeat_task: Optional[asyncio.Task] = None
        self._kafka_consumer: Optional[Consumer] = None
        self._lock = threading.Lock()

    @property
    def client_count(self) -> int:
        """Number of connected clients."""
        return len(self._clients)

    @property
    def is_running(self) -> bool:
        """Check if stream manager is running."""
        return self._running

    def get_stats(self) -> dict:
        """Get stream manager statistics."""
        return {
            "running": self._running,
            "client_count": len(self._clients),
            "clients": [
                {
                    "id": info.client_id,
                    "connected_at": info.connected_at.isoformat(),
                    "topics": list(info.topics)
                }
                for info in self._client_info.values()
            ]
        }

    async def start(self):
        """Start the stream manager and background tasks."""
        if self._running:
            logger.warning("Stream manager already running")
            return

        self._running = True

        # Start background tasks
        self._consumer_task = asyncio.create_task(self._kafka_consumer_loop())
        self._heartbeat_task = asyncio.create_task(self._heartbeat_loop())

        logger.info("Stream manager started")

    async def stop(self):
        """Stop the stream manager and cleanup."""
        self._running = False

        # Cancel tasks
        if self._consumer_task:
            self._consumer_task.cancel()
            try:
                await self._consumer_task
            except asyncio.CancelledError:
                pass

        if self._heartbeat_task:
            self._heartbeat_task.cancel()
            try:
                await self._heartbeat_task
            except asyncio.CancelledError:
                pass

        # Close Kafka consumer
        if self._kafka_consumer:
            self._kafka_consumer.close()
            self._kafka_consumer = None

        # Clear clients
        self._clients.clear()
        self._client_info.clear()

        logger.info("Stream manager stopped")

    def register_client(self, client_id: str, topics: Optional[Set[str]] = None) -> Queue:
        """
        Register a new SSE client.

        Args:
            client_id: Unique client identifier
            topics: Set of topics to subscribe to

        Returns:
            Queue: Message queue for this client
        """
        with self._lock:
            if client_id in self._clients:
                logger.warning(f"Client {client_id} already registered, replacing")

            # å¢åŠ é˜Ÿåˆ—å®¹é‡ï¼Œé¿å…æ•°æ®é‡å¤§æ—¶æ¶ˆæ¯ä¸¢å¤±
            queue = Queue(maxsize=500)  # Buffer up to 500 messages (åŸæ¥æ˜¯100)
            self._clients[client_id] = queue
            self._client_info[client_id] = SSEClient(
                client_id=client_id,
                topics=topics or {"vks-scores", "market-stream"}
            )

            logger.info(f"Client {client_id} registered. Total clients: {len(self._clients)}")
            return queue

    def unregister_client(self, client_id: str):
        """Remove a client from the stream manager."""
        with self._lock:
            if client_id in self._clients:
                del self._clients[client_id]
                del self._client_info[client_id]
                logger.info(f"Client {client_id} unregistered. Total clients: {len(self._clients)}")

    def broadcast(self, event_type: str, data: dict, topic: Optional[str] = None):
        """
        Broadcast a message to all connected clients.

        Args:
            event_type: SSE event type
            data: Message data
            topic: Optional topic filter (only send to clients subscribed to this topic)
        """
        message = {
            "event": event_type,
            "data": data,
            "timestamp": datetime.utcnow().isoformat()
        }

        with self._lock:
            for client_id, queue in self._clients.items():
                # Check topic filter
                if topic:
                    client_info = self._client_info.get(client_id)
                    if client_info and topic not in client_info.topics:
                        continue

                try:
                    queue.put_nowait(message)
                except Exception:
                    # Queue full, skip this message for this client
                    logger.warning(f"Queue full for client {client_id}, dropping message")

    def _parse_vks_scores_message(self, msg) -> dict:
        """
        Parse a message from vks-scores topic.

        Flink SQL outputs raw bytes in this format:
        - key: hashtag as UTF-8 bytes (e.g., b"#NFT")
        - val: VKS score as UTF-8 bytes (e.g., b"58.55")

        Returns:
            dict: Structured VKS data with hashtag and score
        """
        try:
            # Decode key (hashtag)
            key_bytes = msg.key()
            hashtag = key_bytes.decode("utf-8") if key_bytes else "unknown"

            # Decode value (VKS score)
            val_bytes = msg.value()
            vks_score_str = val_bytes.decode("utf-8") if val_bytes else "0"

            # Try to parse as float
            try:
                vks_score = float(vks_score_str)
            except ValueError:
                vks_score = 0.0
                logger.warning(f"Could not parse VKS score: {vks_score_str}")

            return {
                "hashtag": hashtag,
                "vks_score": vks_score,
                "timestamp": datetime.utcnow().isoformat(),
                "source": "flink_sql"
            }
        except Exception as e:
            logger.error(f"Error parsing vks-scores message: {e}")
            return {
                "hashtag": "error",
                "vks_score": 0.0,
                "error": str(e)
            }

    def _calculate_vks_from_market_data(self, data: dict) -> dict:
        """
        Calculate Trend Score from market-stream raw data using adaptive_trend_scorer.
        
        Uses the full 6-dimension scoring system:
        - H (Hotness): çƒ­åº¦
        - V (Velocity): å¢é€Ÿ
        - D (Density): å¯†åº¦
        - F (Feasibility): å¯è¡Œæ€§
        - M (Monetization): å•†ä¸šåŒ–
        - R (Risk): é£é™©
        
        Formula: trend_score = 0.20*H + 0.30*V + 0.15*D + 0.15*F + 0.20*M - 0.25*R
        """
        try:
            # æå–å¹³å°ä¿¡æ¯
            platform = data.get("platform", "unknown")
            
            # æå– hashtag
            hashtag = data.get("hashtag", data.get("tag", "unknown"))
            if hashtag and not hashtag.startswith("#"):
                hashtag = f"#{hashtag}"
            
            # æå–ä½œè€…ä¿¡æ¯
            author = data.get("author", {})
            if isinstance(author, dict):
                author_name = author.get("username") or author.get("nickname") or "unknown"
            else:
                author_name = str(author) if author else "unknown"
            
            # æå–å†…å®¹å¯¹è±¡
            content = data.get("content", {})
            
            # æå–æ ‡é¢˜ (title) - å„å¹³å°å­—æ®µåä¸åŒ
            # TikTok: content.title
            # YouTube: content.title æˆ– rawData.title
            # Instagram: content.title æˆ– caption
            # Twitter: text æˆ– content.text
            # Reddit: title
            title = ""
            if isinstance(content, dict):
                title = content.get("title", "")
            if not title:
                title = (
                    data.get("title", "") or  # é€šç”¨ (Kafka æ ‡å‡†æ ¼å¼)
                    data.get("text", "") or   # Twitter
                    data.get("caption", "") or  # Instagram
                    data.get("name", "")  # Reddit
                )
            # ä» rawData ä¸­å°è¯•è·å–
            raw_data_inner = data.get("rawData", {}) or data.get("raw", {})
            if not title and isinstance(raw_data_inner, dict):
                title = (
                    raw_data_inner.get("title", "") or 
                    raw_data_inner.get("desc", "") or
                    raw_data_inner.get("text", "") or  # Twitter raw
                    raw_data_inner.get("full_text", "")  # Twitter full_text
                )
            
            # è°ƒè¯•æ—¥å¿—
            if platform.lower() == "twitter" and not title:
                logger.warning(f"[DEBUG] Twitter post missing title. Keys: {list(data.keys())}, raw keys: {list(raw_data_inner.keys()) if isinstance(raw_data_inner, dict) else 'N/A'}")
            
            # æå–æè¿° (description) - é€šå¸¸æ˜¯æ›´é•¿çš„æ–‡æœ¬
            description = data.get("description", "")
            if not description and isinstance(content, dict):
                description = content.get("description", "") or content.get("text", "")
            if not description and isinstance(raw_data_inner, dict):
                description = raw_data_inner.get("description", "") or raw_data_inner.get("desc", "")
            # å¦‚æœæ²¡æœ‰ descriptionï¼Œä½¿ç”¨ title ä½œä¸º fallback
            if not description:
                description = title
            
            # æå– URL
            content_url = ""
            cover_url = ""
            
            # ä¼˜å…ˆä»é¡¶å±‚å­—æ®µè·å– (Kafka å‘é€çš„æ ‡å‡†æ ¼å¼)
            content_url = data.get("content_url", "")
            cover_url = data.get("cover_url", "")
            
            # å¦‚æœé¡¶å±‚æ²¡æœ‰ï¼Œä» content å¯¹è±¡è·å–
            if not content_url and isinstance(content, dict):
                content_url = content.get("url", "")
            if not cover_url and isinstance(content, dict):
                cover_url = content.get("coverUrl", "") or content.get("thumbnailUrl", "") or content.get("mediaUrl", "")
            
            # æœ€åå°è¯•å…¶ä»–å­—æ®µ
            if not content_url:
                content_url = data.get("url", "") or data.get("link", "") or data.get("share_url", "")
            if not cover_url:
                cover_url = data.get("coverUrl", "") or data.get("thumbnail", "") or data.get("image", "") or data.get("cover", "")
            
            # æå–å¸–å­ ID
            post_id = data.get("post_id") or data.get("id") or "unknown"
            
            # æ„å»ºçˆ¬è™«æ•°æ®æ ¼å¼ä¾› adaptive_trend_scorer ä½¿ç”¨
            crawl_item = {
                "platform": platform,
                "id": post_id,
                "stats": data.get("stats", {
                    "views": data.get("views", 0) or 0,
                    "likes": data.get("likes", 0) or 0,
                    "comments": data.get("comments", 0) or 0,
                    "shares": data.get("shares", 0) or 0,
                    "saves": data.get("saves", 0) or 0,
                    # Reddit ç‰¹æ®Šå­—æ®µ
                    "upvotes": data.get("upvotes", 0) or 0,
                    "downvotes": data.get("downvotes", 0) or 0,
                    "score": data.get("score", 0) or 0,
                })
            }
            
            # å¦‚æœ stats å­—æ®µä¸å­˜åœ¨ï¼Œä»é¡¶å±‚æå–
            if not data.get("stats"):
                crawl_item["stats"] = {
                    "views": data.get("views", 0) or 0,
                    "likes": data.get("likes", 0) or 0,
                    "comments": data.get("comments", 0) or 0,
                    "shares": data.get("shares", 0) or 0,
                    "saves": data.get("saves", 0) or 0,
                    "upvotes": data.get("upvotes", 0) or 0,
                    "downvotes": data.get("downvotes", 0) or 0,
                    "score": data.get("score", 0) or 0,
                }
            
            # ä½¿ç”¨ adaptive_trend_scorer è®¡ç®—å®Œæ•´çš„ Trend Score
            score_result = adaptive_trend_scorer.compute_from_crawl_item(
                item=crawl_item,
                keyword=hashtag.lstrip("#")
            )
            
            logger.info(f"ğŸ“Š Adaptive Trend Score - platform: {platform}, keyword: {hashtag}, "
                        f"trend_score: {score_result.get('trend_score')}, "
                        f"H={score_result.get('H')}, V={score_result.get('V')}, "
                        f"D={score_result.get('D')}, F={score_result.get('F')}, "
                        f"M={score_result.get('M')}, R={score_result.get('R')}")
            
            # æ„å»ºç»“æœ
            result = {
                "hashtag": hashtag,
                "vks_score": score_result.get("trend_score", 0),  # å…¼å®¹æ—§å­—æ®µå
                "trend_score": score_result.get("trend_score", 0),
                "platform": platform,
                "post_id": post_id,
                "author": author_name,
                "title": title[:200] if title else "",  # æ–°å¢ï¼šå¸–å­æ ‡é¢˜
                "description": description[:500] if description else "",
                "content_url": content_url,
                "cover_url": cover_url,
                "timestamp": datetime.utcnow().isoformat(),
                "source": "adaptive_trend_scorer",
                # 6 ç»´åº¦åˆ†æ•°
                "dimensions": {
                    "H": score_result.get("H", 0),  # çƒ­åº¦
                    "V": score_result.get("V", 0),  # å¢é€Ÿ
                    "D": score_result.get("D", 0),  # å¯†åº¦
                    "F": score_result.get("F", 0),  # å¯è¡Œæ€§
                    "M": score_result.get("M", 0),  # å•†ä¸šåŒ–
                    "R": score_result.get("R", 0),  # é£é™©
                },
                # å…ƒæ•°æ®
                "lifecycle": score_result.get("lifecycle", "unknown"),
                "priority": score_result.get("priority", "P3"),
                "agent_ready": score_result.get("agent_ready", False),
                "category": score_result.get("category", "general"),
                # åŸå§‹æŒ‡æ ‡
                "metrics": score_result.get("raw_metrics", {
                    "views": crawl_item["stats"].get("views", 0),
                    "likes": crawl_item["stats"].get("likes", 0),
                    "comments": crawl_item["stats"].get("comments", 0),
                    "shares": crawl_item["stats"].get("shares", 0),
                    "saves": crawl_item["stats"].get("saves", 0),
                })
            }
            
            # æ™ºèƒ½å­˜å‚¨ï¼šå»é‡ + æ›´æ–° + èšåˆè®¡ç®—
            try:
                # 1. å­˜å‚¨/æ›´æ–°å¸–å­æ•°æ®ï¼ˆè‡ªåŠ¨å»é‡ï¼‰ï¼ŒåŒ…å«å•å¸–åˆ†æ•°
                is_new, prev_stats = smart_history_store.upsert_post(
                    platform=platform,
                    tag=hashtag.lstrip("#"),
                    post_id=post_id,
                    stats=crawl_item["stats"],
                    author=author_name,
                    title=title[:200] if title else "",
                    description=description[:200] if description else "",
                    content_url=content_url,
                    cover_url=cover_url,
                    post_created_at=data.get("created_at", ""),
                    # å•å¸–åˆ†æ•°
                    trend_score=score_result.get("trend_score", 0),
                    dimensions=result["dimensions"],
                    lifecycle=score_result.get("lifecycle", "unknown"),
                    priority=score_result.get("priority", "P3")
                )
                
                # 2. è·å–è¯¥ tag çš„èšåˆæ•°æ®ï¼ˆåŒ…å«æ–°é²œåº¦ï¼‰
                aggregated = smart_history_store.get_tag_aggregated_stats(
                    platform, hashtag, 
                    current_batch_size=20  # æ¯æ¬¡çˆ¬å–çº¦ 20 æ¡
                )
                
                # 3. ä½¿ç”¨èšåˆæ•°æ®é‡æ–°è®¡ç®— Trend Scoreï¼ˆå¸¦å¢é•¿ç‡ + æ–°é²œåº¦ï¼‰
                from app.services.adaptive_trend_scorer import compute_adaptive_trend_score
                aggregated_score = compute_adaptive_trend_score(
                    keyword=hashtag.lstrip("#"),
                    platform_str=platform,
                    raw_stats=aggregated["current"],
                    prev_raw_stats=aggregated["previous"] if aggregated["previous"]["views"] > 0 else None,
                    posts=aggregated["post_count"],
                    freshness_rate=aggregated.get("freshness_rate", 0.5),
                    new_posts=aggregated.get("new_posts", 0),
                    activity_level=aggregated.get("activity_level", "unknown")
                )
                
                # 4. ä¿å­˜ tag èšåˆåˆ†æ•°
                smart_history_store.save_tag_score(
                    platform=platform,
                    tag=hashtag,
                    aggregated_stats=aggregated,
                    trend_score=aggregated_score["trend_score"],
                    dimensions={
                        "H": aggregated_score["H"],
                        "V": aggregated_score["V"],
                        "D": aggregated_score["D"],
                        "F": aggregated_score["F"],
                        "M": aggregated_score["M"],
                        "R": aggregated_score["R"]
                    },
                    lifecycle=aggregated_score["lifecycle"],
                    priority=aggregated_score["priority"]
                )
                
                # æ›´æ–°è¿”å›ç»“æœä¸ºèšåˆåˆ†æ•°
                result["trend_score"] = aggregated_score["trend_score"]
                result["vks_score"] = aggregated_score["trend_score"]
                result["dimensions"] = {
                    "H": aggregated_score["H"],
                    "V": aggregated_score["V"],
                    "D": aggregated_score["D"],
                    "F": aggregated_score["F"],
                    "M": aggregated_score["M"],
                    "R": aggregated_score["R"]
                }
                result["lifecycle"] = aggregated_score["lifecycle"]
                result["priority"] = aggregated_score["priority"]
                result["is_new_post"] = is_new
                result["post_count"] = aggregated["post_count"]
                result["new_posts_count"] = aggregated["new_posts"]
                
                # æ–°å¢ï¼šæ´»è·ƒåº¦ä¿¡æ¯
                result["activity"] = {
                    "freshness_rate": aggregated.get("freshness_rate", 0),
                    "activity_level": aggregated.get("activity_level", "unknown"),
                    "new_posts": aggregated.get("new_posts", 0),
                }
                
                status = "NEW" if is_new else "UPDATED"
                freshness = aggregated.get("freshness_rate", 0)
                activity = aggregated.get("activity_level", "?")
                logger.info(f"ğŸ“¦ [{status}] {platform}/{hashtag}: posts={aggregated['post_count']}, "
                           f"score={aggregated_score['trend_score']}, V={aggregated_score['V']:.2f}, "
                           f"D={aggregated_score['D']:.2f}, freshness={freshness:.0%} ({activity})")
                
            except Exception as e:
                logger.warning(f"Failed to smart store: {e}", exc_info=True)
            
            return result
        except Exception as e:
            logger.error(f"Error calculating Trend Score from market data: {e}", exc_info=True)
            return {
                "hashtag": data.get("hashtag", data.get("tag", "unknown")),
                "vks_score": 0.0,
                "trend_score": 0.0,
                "platform": data.get("platform", "unknown"),
                "error": str(e),
                "source": "error_fallback"
            }

    async def _kafka_consumer_loop(self):
        """Background task to consume Kafka messages."""
        loop = asyncio.get_event_loop()
        
        try:
            # Initialize consumer with unique group id to get all messages
            import time
            unique_group_id = f"adalpha-sse-stream-{int(time.time())}"
            
            # åœ¨çº¿ç¨‹æ± ä¸­åˆå§‹åŒ– consumerï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            self._kafka_consumer = await loop.run_in_executor(
                None, 
                lambda: kafka_client.get_consumer(group_id=unique_group_id)
            )
            await loop.run_in_executor(
                None,
                lambda: self._kafka_consumer.subscribe(["vks-scores", "market-stream"])
            )

            logger.info(f"Kafka consumer started for SSE streaming (group: {unique_group_id})")
            logger.info("Subscribed to topics: vks-scores, market-stream")

            while self._running:
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé˜»å¡çš„ poll æ“ä½œ
                    msg = await loop.run_in_executor(
                        None,
                        lambda: self._kafka_consumer.poll(timeout=1.0)
                    )

                    if msg is None:
                        # æ²¡æœ‰æ¶ˆæ¯ï¼Œç»§ç»­å¾ªç¯
                        continue

                    if msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            logger.debug(f"Reached end of partition for {msg.topic()}")
                            continue
                        logger.error(f"Kafka error: {msg.error()}")
                        continue

                    # æ”¶åˆ°æ¶ˆæ¯ï¼Œæ‰“å°æ—¥å¿—
                    logger.info(f"ğŸ“¨ Received message from topic: {msg.topic()}, partition: {msg.partition()}, offset: {msg.offset()}")

                    # Parse message
                    topic = msg.topic()

                    # Handle vks-scores topic specially (Flink SQL outputs raw bytes)
                    if topic == "vks-scores":
                        data = self._parse_vks_scores_message(msg)
                        event_type = "vks_update"
                        # Broadcast to clients
                        logger.info(f"ğŸ“¤ Broadcasting {event_type} to {self.client_count} clients: {data}")
                        self.broadcast(event_type, data, topic)
                        
                    elif topic == "market-stream":
                        # market-stream has JSON format from crawler
                        value = msg.value().decode("utf-8")
                        try:
                            raw_data = json.loads(value)
                        except json.JSONDecodeError:
                            raw_data = {"raw": value}

                        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦è¢« Kafka ä¸­é—´ä»¶åŒ…è£…
                        # å¦‚æœæ•°æ®è¢«åŒ…è£…ï¼Œkeys ä¼šæ˜¯ ['event_id', 'event_type', 'data', 'source', 'ingested_at']
                        # çœŸæ­£çš„ç¤¾äº¤æ•°æ®è—åœ¨ 'data' å­—æ®µé‡Œ
                        if 'data' in raw_data and isinstance(raw_data.get('data'), dict):
                            # æ•°æ®è¢«åŒ…è£…äº†ï¼Œè§£åŒ…è·å–çœŸå®æ•°æ®
                            actual_data = raw_data['data']
                            logger.info(f"ğŸ“¦ Unwrapped packaged data: event_type={raw_data.get('event_type')}, source={raw_data.get('source')}")
                        else:
                            # æ•°æ®æœªè¢«åŒ…è£…ï¼Œç›´æ¥ä½¿ç”¨
                            actual_data = raw_data

                        # ğŸ” è°ƒè¯•ï¼šæ‰“å°è§£åŒ…åçš„æ•°æ®ç»“æ„
                        data_type = actual_data.get("type", "NO_TYPE")
                        data_keys = list(actual_data.keys())[:10]  # å‰10ä¸ªkey
                        logger.info(f"ğŸ“¥ market-stream data: type={data_type}, keys={data_keys}")

                        # 1. å‘é€åŸå§‹ trend_update äº‹ä»¶ï¼ˆä½¿ç”¨è§£åŒ…åçš„æ•°æ®ï¼‰
                        logger.info(f"ğŸ“¤ Broadcasting trend_update to {self.client_count} clients")
                        self.broadcast("trend_update", actual_data, topic)

                        # 2. è®¡ç®— VKS å¹¶å‘é€ vks_update äº‹ä»¶
                        # ä½¿ç”¨è§£åŒ…åçš„æ•°æ®åˆ¤æ–­
                        has_social_data = (
                            actual_data.get("type") == "social_post" or
                            actual_data.get("platform") or
                            actual_data.get("hashtag") or
                            actual_data.get("tag")
                        )

                        if has_social_data:
                            vks_data = self._calculate_vks_from_market_data(actual_data)
                            logger.info(f"ğŸ“¤ Broadcasting vks_update (calculated) to {self.client_count} clients: hashtag={vks_data.get('hashtag')}, score={vks_data.get('trend_score')}")
                            self.broadcast("vks_update", vks_data, "vks-scores")
                        else:
                            logger.warning(f"âš ï¸ Skipping vks_update: no social data fields found in {data_keys}")
                    else:
                        value = msg.value().decode("utf-8")
                        try:
                            data = json.loads(value)
                        except json.JSONDecodeError:
                            data = {"raw": value}
                        event_type = "message"
                        # Broadcast to clients
                        logger.info(f"ğŸ“¤ Broadcasting {event_type} to {self.client_count} clients: {data}")
                        self.broadcast(event_type, data, topic)

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Error in Kafka consumer loop: {e}", exc_info=True)
                    await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"Failed to start Kafka consumer: {e}", exc_info=True)

    async def _heartbeat_loop(self):
        """Send periodic heartbeat to keep SSE connections alive."""
        while self._running:
            try:
                await asyncio.sleep(15)  # Heartbeat every 15 seconds

                if self._clients:
                    self.broadcast("heartbeat", {
                        "type": "ping",
                        "client_count": len(self._clients),
                        "timestamp": datetime.utcnow().isoformat()
                    })
                    logger.debug(f"Sent heartbeat to {len(self._clients)} clients")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in heartbeat loop: {e}")


# Global singleton
stream_manager = StreamManager()


async def sse_event_generator(
    client_id: str,
    topics: Optional[Set[str]] = None
) -> AsyncGenerator[str, None]:
    """
    Async generator for SSE events.

    Args:
        client_id: Unique client identifier
        topics: Optional set of topics to subscribe to

    Yields:
        SSE formatted event strings
    """
    # Ensure stream manager is running
    if not stream_manager.is_running:
        await stream_manager.start()

    # Register client and get queue
    queue = stream_manager.register_client(client_id, topics)

    # Send initial connection event
    yield format_sse_event("connected", {
        "client_id": client_id,
        "message": "Connected to ADALPHA real-time stream"
    })

    try:
        while True:
            try:
                # Non-blocking queue check
                message = queue.get_nowait()
                yield format_sse_event(message["event"], message["data"])
            except Empty:
                # No message available, yield nothing and wait
                await asyncio.sleep(0.1)

    except asyncio.CancelledError:
        pass
    except GeneratorExit:
        pass
    finally:
        stream_manager.unregister_client(client_id)


def format_sse_event(event_type: str, data: dict, event_id: Optional[str] = None) -> str:
    """
    Format data as an SSE event string.

    Args:
        event_type: Event type name
        data: Data to send
        event_id: Optional event ID for reconnection

    Returns:
        SSE formatted string
    """
    lines = []

    if event_id:
        lines.append(f"id: {event_id}")

    lines.append(f"event: {event_type}")
    lines.append(f"data: {json.dumps(data)}")
    lines.append("")  # Empty line to end event

    return "\n".join(lines) + "\n"
