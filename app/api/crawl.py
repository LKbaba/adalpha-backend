"""
Crawl API Endpoints - çˆ¬è™«è§¦å‘æ¥å£

æä¾›æ‰‹åŠ¨è§¦å‘çˆ¬è™«çš„ APIï¼Œæ”¯æŒå‰ç«¯åœ¨åŠ è½½é¡µé¢æ—¶è§¦å‘çˆ¬å–æµç¨‹ã€‚
é€šè¿‡è°ƒç”¨ spider6p æœåŠ¡å™¨ (http://localhost:8001) æ¥æ‰§è¡Œçˆ¬å–ã€‚
"""

import logging
import asyncio
import aiohttp
from typing import Optional, List
from datetime import datetime
from fastapi import APIRouter, Query, BackgroundTasks
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/crawl", tags=["Crawler Control"])

# Spider6P æœåŠ¡å™¨åœ°å€
SPIDER_SERVER_URL = "http://localhost:8001"


# === State ===
class CrawlState:
    """çˆ¬è™«çŠ¶æ€ç®¡ç†"""

    def __init__(self):
        self.is_running = False
        self.last_run: Optional[datetime] = None
        self.last_result: Optional[dict] = None
        self.progress: int = 0
        self.current_platform: str = ""
        self.error: Optional[str] = None
        self.trigger_count: int = 0  # è§¦å‘è®¡æ•°å™¨


crawl_state = CrawlState()


# === Response Models ===


class CrawlStatusResponse(BaseModel):
    """çˆ¬è™«çŠ¶æ€å“åº”"""

    is_running: bool
    progress: int = Field(ge=0, le=100, description="è¿›åº¦ç™¾åˆ†æ¯”")
    current_platform: str = ""
    last_run: Optional[str] = None
    error: Optional[str] = None


class CrawlTriggerResponse(BaseModel):
    """è§¦å‘çˆ¬è™«å“åº”"""

    success: bool
    message: str
    status: CrawlStatusResponse


# === Background Task ===


async def run_crawler_task(tags: List[str], mock: bool = True):
    """
    åå°è¿è¡Œçˆ¬è™«ä»»åŠ¡

    é€šè¿‡è°ƒç”¨ spider6p æœåŠ¡å™¨çš„ API æ¥æ‰§è¡Œçˆ¬å–
    
    Args:
        tags: æ ‡ç­¾åˆ—è¡¨
        mock: æ˜¯å¦ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆä½¿ç”¨å·²æœ‰æ•°æ®ï¼Œä¸æ¶ˆè€— APIï¼‰
    """
    global crawl_state

    try:
        crawl_state.is_running = True
        crawl_state.progress = 10
        crawl_state.error = None
        crawl_state.current_platform = "CONNECTING"

        mode_text = "Mock æ¨¡å¼" if mock else "çœŸå®çˆ¬å–"
        logger.info(f"Triggering spider6p ({mode_text}) with tags: {tags}")

        # è°ƒç”¨ spider6p æœåŠ¡å™¨
        async with aiohttp.ClientSession() as session:
            crawl_state.current_platform = "CRAWLING"
            crawl_state.progress = 20

            try:
                # ç»Ÿä¸€è°ƒç”¨ /run ç«¯ç‚¹ï¼Œç”±çˆ¬è™«ç«¯çš„ config.useMock å†³å®šæ¨¡å¼
                url = f"{SPIDER_SERVER_URL}/run"
                payload = {}
                logger.info(f"Calling spider6p: POST {url}")

                async with session.post(
                    url, json=payload, timeout=aiohttp.ClientTimeout(total=180)
                ) as response:
                    result = await response.json()

                    if response.status == 200 and result.get("success"):
                        crawl_state.progress = 100
                        crawl_state.current_platform = "COMPLETE"
                        crawl_state.last_result = result
                        logger.info(f"Spider6p completed successfully ({mode_text}): {result}")
                    elif response.status == 409:
                        # çˆ¬è™«æ­£åœ¨è¿è¡Œ
                        crawl_state.current_platform = "ALREADY_RUNNING"
                        crawl_state.error = result.get("message", "Crawler already running")
                        logger.warning(f"Spider6p already running: {result}")
                    else:
                        crawl_state.error = result.get("message", "Unknown error")
                        crawl_state.current_platform = "ERROR"
                        logger.error(f"Spider6p failed: {result}")

            except aiohttp.ClientConnectorError:
                # çˆ¬è™«æœåŠ¡å™¨æœªå¯åŠ¨
                crawl_state.error = "Spider6p server not running (localhost:8001)"
                crawl_state.current_platform = "SERVER_OFFLINE"
                logger.error(
                    "Spider6p server not running. Start it with: cd spider6p && npm run server"
                )

            except asyncio.TimeoutError:
                crawl_state.error = "Crawler timeout (180s)"
                crawl_state.current_platform = "TIMEOUT"
                logger.error("Spider6p request timeout")

        crawl_state.last_run = datetime.utcnow()

    except Exception as e:
        logger.error(f"Crawl task failed: {e}", exc_info=True)
        crawl_state.error = str(e)
        crawl_state.current_platform = "ERROR"

    finally:
        crawl_state.is_running = False


# === Endpoints ===


@router.get("/status", response_model=CrawlStatusResponse)
async def get_crawl_status():
    """
    è·å–çˆ¬è™«çŠ¶æ€

    è¿”å›å½“å‰çˆ¬è™«çš„è¿è¡ŒçŠ¶æ€ã€è¿›åº¦å’Œæœ€åè¿è¡Œæ—¶é—´ã€‚
    åŒæ—¶ä¼šå°è¯•ä» spider6p æœåŠ¡å™¨è·å–å®æ—¶çŠ¶æ€ã€‚
    """
    # å°è¯•ä» spider6p è·å–å®æ—¶çŠ¶æ€
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{SPIDER_SERVER_URL}/status",
                timeout=aiohttp.ClientTimeout(total=5),
            ) as response:
                if response.status == 200:
                    spider_status = await response.json()
                    return CrawlStatusResponse(
                        is_running=spider_status.get("running", False),
                        progress=100 if spider_status.get("lastResult") else 0,
                        current_platform=spider_status.get("config", {}).get(
                            "platforms", [""]
                        )[0]
                        if spider_status.get("running")
                        else "IDLE",
                        last_run=spider_status.get("lastRun"),
                        error=None,
                    )
    except Exception:
        pass  # å¦‚æœæ— æ³•è¿æ¥ï¼Œä½¿ç”¨æœ¬åœ°çŠ¶æ€

    return CrawlStatusResponse(
        is_running=crawl_state.is_running,
        progress=crawl_state.progress,
        current_platform=crawl_state.current_platform,
        last_run=crawl_state.last_run.isoformat() if crawl_state.last_run else None,
        error=crawl_state.error,
    )


@router.post("/trigger", response_model=CrawlTriggerResponse)
async def trigger_crawl(
    background_tasks: BackgroundTasks,
    tags: Optional[str] = Query(
        default="AI,trending", description="é€—å·åˆ†éš”çš„æ ‡ç­¾åˆ—è¡¨"
    ),
    mock: bool = Query(
        default=True, description="æ˜¯å¦ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆä½¿ç”¨å·²æœ‰æ•°æ®ï¼Œä¸æ¶ˆè€— API è´¹ç”¨ï¼‰"
    ),
):
    """
    è§¦å‘çˆ¬è™«

    å¯åŠ¨åå°çˆ¬è™«ä»»åŠ¡ï¼Œè¿”å›ç«‹å³å“åº”ã€‚
    ä½¿ç”¨ /status ç«¯ç‚¹è½®è¯¢è¿›åº¦ã€‚

    å‚æ•°:
    - tags: é€—å·åˆ†éš”çš„æ ‡ç­¾åˆ—è¡¨
    - mock: æ˜¯å¦ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆé»˜è®¤ Trueï¼Œä½¿ç”¨å·²æœ‰æ•°æ®ä¸æ¶ˆè€— APIï¼‰

    æ³¨æ„ï¼šéœ€è¦å…ˆå¯åŠ¨ spider6p æœåŠ¡å™¨ï¼š
    ```
    cd spider6p && npm run server
    ```

    ç¤ºä¾‹:
    - POST /api/crawl/trigger?tags=AI,NFT&mock=true  (Mock æ¨¡å¼)
    - POST /api/crawl/trigger?tags=AI,NFT&mock=false (çœŸå®çˆ¬å–)
    """
    if crawl_state.is_running:
        return CrawlTriggerResponse(
            success=False,
            message="Crawler is already running",
            status=CrawlStatusResponse(
                is_running=True,
                progress=crawl_state.progress,
                current_platform=crawl_state.current_platform,
                last_run=crawl_state.last_run.isoformat()
                if crawl_state.last_run
                else None,
                error=crawl_state.error,
            ),
        )

    tag_list = [t.strip() for t in tags.split(",") if t.strip()]

    # å¢åŠ è§¦å‘è®¡æ•°
    crawl_state.trigger_count += 1
    logger.info(f"ğŸ”¢ Crawl trigger count: {crawl_state.trigger_count}, mock={mock}")

    # å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(run_crawler_task, tag_list, mock)

    mode_text = "Mock æ¨¡å¼" if mock else "çœŸå®çˆ¬å–"
    return CrawlTriggerResponse(
        success=True,
        message=f"Crawler started ({mode_text}) for tags: {tag_list}",
        status=CrawlStatusResponse(
            is_running=True,
            progress=0,
            current_platform="INITIALIZING",
            last_run=crawl_state.last_run.isoformat() if crawl_state.last_run else None,
            error=None,
        ),
    )


@router.post("/stop")
async def stop_crawl():
    """
    åœæ­¢çˆ¬è™«

    æ³¨æ„ï¼šspider6p ç›®å‰ä¸æ”¯æŒä¸­é€”åœæ­¢ï¼Œæ­¤æ¥å£ä»…é‡ç½®æœ¬åœ°çŠ¶æ€ã€‚
    """
    crawl_state.is_running = False
    crawl_state.current_platform = "STOPPED"

    return {"success": True, "message": "Local state reset. Note: spider6p cannot be stopped mid-crawl."}


@router.get("/trigger-count")
async def get_trigger_count():
    """
    è·å–çˆ¬è™«è§¦å‘æ¬¡æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    """
    return {
        "trigger_count": crawl_state.trigger_count,
        "is_running": crawl_state.is_running,
        "last_run": crawl_state.last_run.isoformat() if crawl_state.last_run else None,
    }


@router.post("/reset-count")
async def reset_trigger_count():
    """
    é‡ç½®è§¦å‘è®¡æ•°å™¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    """
    crawl_state.trigger_count = 0
    return {"success": True, "message": "Trigger count reset to 0"}


@router.get("/health")
async def check_spider_health():
    """
    æ£€æŸ¥ spider6p æœåŠ¡å™¨å¥åº·çŠ¶æ€
    """
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{SPIDER_SERVER_URL}/health",
                timeout=aiohttp.ClientTimeout(total=5),
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        "spider_server": "online",
                        "url": SPIDER_SERVER_URL,
                        "timestamp": data.get("timestamp"),
                    }
    except aiohttp.ClientConnectorError:
        return {
            "spider_server": "offline",
            "url": SPIDER_SERVER_URL,
            "error": "Cannot connect to spider6p server",
            "hint": "Start it with: cd spider6p && npm run server",
        }
    except Exception as e:
        return {
            "spider_server": "error",
            "url": SPIDER_SERVER_URL,
            "error": str(e),
        }
